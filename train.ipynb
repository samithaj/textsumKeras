{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "FN = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "# os.environ['THEANO_FLAGS'] = 'device=cpu,floatX=float32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.0-rc1'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import keras\n",
    "tf.__version__\n",
    "#tf.contrib.keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FN0 = 'vocabulary-embedding'\n",
    "FN1 = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlend=25 # 0 - if we dont want to use description at all\n",
    "maxlenh=25\n",
    "maxlen = maxlend + maxlenh\n",
    "rnn_size = 512 # must be same as 160330-word-gen\n",
    "rnn_layers = 3  # match FN1\n",
    "batch_norm=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "activation_rnn_size = 40 if maxlend else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "seed=42\n",
    "p_W, p_U, p_dense, p_emb, weight_decay = 0, 0, 0, 0, 0\n",
    "optimizer = 'adam'\n",
    "LR = 1e-4\n",
    "batch_size=64\n",
    "nflips=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_train_samples = 600000\n",
    "nb_val_samples = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pickle\n",
    "\n",
    "with open('data/%s.pkl'%FN0, 'rb') as fp:\n",
    "    embedding, idx2word, word2idx, glove_idx2idx = pickle.load(fp)\n",
    "vocab_size, embedding_size = embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/%s.data.pkl'%FN0, 'rb') as fp:\n",
    "    X, Y = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_unknown_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples 1000000 1000000\n",
      "dimension of embedding space for words 100\n",
      "vocabulary size 200000 the last 10 words can be used as place holders for unknown/oov words\n",
      "total number of different words 2002347 2002347\n",
      "number of words outside vocabulary which we can substitue using glove similarity 126962\n",
      "number of words that will be regarded as unknonw(unk)/out-of-vocabulary(oov) 1675385\n"
     ]
    }
   ],
   "source": [
    "print ('number of examples',len(X),len(Y))\n",
    "print ('dimension of embedding space for words',embedding_size)\n",
    "print ('vocabulary size', vocab_size, 'the last %d words can be used as place holders for unknown/oov words'%nb_unknown_words)\n",
    "print ('total number of different words',len(idx2word), len(word2idx))\n",
    "print ('number of words outside vocabulary which we can substitue using glove similarity', len(glove_idx2idx))\n",
    "print ('number of words that will be regarded as unknonw(unk)/out-of-vocabulary(oov)',len(idx2word)-vocab_size-len(glove_idx2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(nb_unknown_words):\n",
    "    idx2word[vocab_size-1-i] = '<%d>'%i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oov0 = vocab_size-nb_unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(oov0, len(idx2word)):\n",
    "    idx2word[i] = idx2word[i]+'^'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krotovd/anaconda/envs/tf/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(900000, 900000, 100000, 100000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=nb_val_samples, random_state=seed)\n",
    "len(X_train), len(Y_train), len(X_test), len(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del X\n",
    "del Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "empty = 0\n",
    "eos = 1\n",
    "idx2word[empty] = '_'\n",
    "idx2word[eos] = '~'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "import random, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prt(label, x):\n",
    "    print (label+':')\n",
    "    for w in x:\n",
    "        print (idx2word[w])\n",
    "    #print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H:\n",
      "why\n",
      "procter\n",
      "gamble\n",
      "will\n",
      "make\n",
      "major\n",
      "comeback\n",
      "D:\n",
      "new\n",
      "york\n",
      "thestreet\n",
      "procter\n",
      "gamble\n",
      "going\n",
      "through\n",
      "rough\n",
      "patch\n",
      "and\n",
      "because\n",
      "that\n",
      "dividend\n",
      "yield\n",
      "might\n",
      "look\n",
      "like\n",
      "warning\n",
      "sign\n",
      "not\n",
      "rich\n",
      "dividend\n",
      "but\n",
      "now\n",
      "the\n",
      "time\n",
      "buy\n",
      "shares\n",
      "dividend\n",
      "aristocrat\n",
      "because\n",
      "this\n",
      "stock\n",
      "likely\n",
      "rise\n",
      "higher\n",
      "the\n",
      "company\n",
      "turnaround\n",
      "efforts\n",
      "although\n",
      "procter\n",
      "gamble\n",
      "makes\n",
      "products\n",
      "that\n",
      "fill\n",
      "pantries\n",
      "and\n",
      "medicine\n",
      "cabinets\n",
      "the\n",
      "world\n",
      "over\n",
      "its\n",
      "sales\n",
      "and\n",
      "profits\n",
      "took\n",
      "substantial\n",
      "hit\n",
      "the\n",
      "most\n",
      "recent\n",
      "earnings\n",
      "report\n",
      "and\n",
      "has\n",
      "struggled\n",
      "generally\n",
      "for\n",
      "the\n",
      "last\n",
      "few\n",
      "years\n",
      "net\n",
      "income\n",
      "dropped\n",
      "million\n",
      "quarter\n",
      "ended\n",
      "june\n",
      "from\n",
      "billion\n",
      "the\n",
      "previous\n",
      "quarter\n",
      "sales\n",
      "declined\n",
      "billion\n",
      "from\n",
      "from\n",
      "billion\n",
      "the\n",
      "strong\n",
      "dollar\n",
      "and\n",
      "recent\n",
      "acquisitions\n",
      "accounted\n",
      "for\n",
      "some\n",
      "the\n",
      "disappointment\n",
      "but\n",
      "even\n",
      "accounting\n",
      "for\n",
      "those\n",
      "sales\n",
      "rose\n",
      "just\n",
      "showing\n",
      "the\n",
      "company\n",
      "widely\n",
      "publicized\n",
      "turnaround\n",
      "strategy\n",
      "based\n",
      "fewer\n",
      "but\n",
      "faster-growing\n",
      "products\n",
      "has\n",
      "n't\n",
      "gotten\n",
      "much\n",
      "traction\n",
      "shares\n",
      "the\n",
      "firm\n",
      "were\n",
      "trading\n",
      "above\n",
      "per\n",
      "share\n",
      "recently\n",
      "january\n",
      "but\n",
      "have\n",
      "since\n",
      "slumped\n",
      "around\n",
      "near\n",
      "the\n",
      "bottom\n",
      "their\n",
      "trading\n",
      "range\n",
      "the\n",
      "silver\n",
      "lining\n",
      "the\n",
      "stock\n",
      "trading\n",
      "around\n",
      "times\n",
      "earnings\n",
      "estimates\n",
      "for\n",
      "the\n",
      "next\n",
      "year\n",
      "making\n",
      "bargain\n",
      "the\n",
      "current\n",
      "market\n",
      "the\n",
      "gold\n",
      "lining\n",
      "average\n",
      "analysts\n",
      "are\n",
      "expecting\n",
      "increase\n",
      "earnings\n",
      "per\n",
      "share\n",
      "between\n",
      "the\n",
      "current\n",
      "fiscal\n",
      "year\n",
      "and\n",
      "the\n",
      "next\n",
      "analysts\n",
      "predict\n",
      "eps\n",
      "will\n",
      "rise\n",
      "from\n",
      "the\n",
      "current\n",
      "fiscal\n",
      "year\n",
      "which\n",
      "ends\n",
      "june\n",
      "the\n",
      "next\n",
      "fiscal\n",
      "year\n",
      "analysts\n",
      "also\n",
      "say\n",
      "there\n",
      "should\n",
      "plenty\n",
      "cash\n",
      "raise\n",
      "the\n",
      "dividend\n",
      "beyond\n",
      "financial\n",
      "predictions\n",
      "know\n",
      "the\n",
      "company\n",
      "has\n",
      "history\n",
      "overcoming\n",
      "challenges\n",
      "currently\n",
      "the\n",
      "final\n",
      "stages\n",
      "five-year\n",
      "restructuring\n",
      "effort\n",
      "aimed\n",
      "cutting\n",
      "almost\n",
      "billion\n",
      "big\n",
      "part\n",
      "these\n",
      "savings\n",
      "came\n",
      "from\n",
      "cutting\n",
      "nonunionized^\n",
      "jobs\n",
      "the\n",
      "savings\n",
      "have\n",
      "helped\n",
      "the\n",
      "company\n",
      "deal\n",
      "with\n",
      "rising\n",
      "raw\n",
      "material\n",
      "costs\n",
      "and\n",
      "compete\n",
      "with\n",
      "cheaper\n",
      "generic\n",
      "products\n",
      "the\n",
      "corporation\n",
      "managers\n",
      "also\n",
      "have\n",
      "n't\n",
      "been\n",
      "shy\n",
      "about\n",
      "letting\n",
      "some\n",
      "product\n",
      "lines\n",
      "procter\n",
      "gamble\n",
      "sold\n",
      "its\n",
      "duracell\n",
      "battery\n",
      "brand\n",
      "warren\n",
      "buffett\n",
      "berkshire\n",
      "hathaway\n",
      "also\n",
      "sold\n",
      "collection\n",
      "beauty-related^\n",
      "products\n",
      "including\n",
      "such\n",
      "well-known\n",
      "brands\n",
      "such\n",
      "cover\n",
      "girl\n",
      "and\n",
      "max\n",
      "factor\n",
      "coty\n",
      "for\n",
      "more\n",
      "than\n",
      "billion\n",
      "next\n",
      "last\n"
     ]
    }
   ],
   "source": [
    "i = 334\n",
    "prt('H',Y_train[i])\n",
    "prt('D',X_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H:\n",
      "ann7\n",
      "christina\n",
      "aguilera\n",
      "dumped\n",
      "its\n",
      "south\n",
      "african\n",
      "the\n",
      "year\n",
      "awards\n",
      "because\n",
      "she\n",
      "judging\n",
      "the\n",
      "voice\n",
      "D:\n",
      "ann7\n",
      "dstv\n",
      "statement\n",
      "now\n",
      "says\n",
      "christina\n",
      "aguilera\n",
      "dumped\n",
      "the\n",
      "south\n",
      "african\n",
      "news\n",
      "channel\n",
      "upcoming\n",
      "south\n",
      "african\n",
      "the\n",
      "year\n",
      "awards\n",
      "because\n",
      "she\n",
      "judging\n",
      "the\n",
      "voice\n",
      "the\n",
      "problem\n",
      "that\n",
      "someone\n",
      "appears\n",
      "lying\n",
      "the\n",
      "judges\n",
      "for\n",
      "the\n",
      "upcoming\n",
      "season\n",
      "the\n",
      "voice\n",
      "nbc\n",
      "starting\n",
      "september\n",
      "the\n",
      "united\n",
      "states\n",
      "were\n",
      "announced\n",
      "back\n",
      "july\n",
      "already\n",
      "and\n",
      "christina\n",
      "aguilera\n",
      "was\n",
      "replaced\n",
      "gwen\n",
      "stefani\n",
      "who\n",
      "will\n",
      "judging\n",
      "for\n",
      "her\n",
      "second\n",
      "season\n",
      "update\n",
      "september\n",
      "christina\n",
      "aguilera\n",
      "judge\n",
      "the\n",
      "season\n",
      "the\n",
      "voice\n",
      "which\n",
      "will\n",
      "broadcast\n",
      "and\n",
      "although\n",
      "the\n",
      "season\n",
      "just\n",
      "started\n",
      "america\n",
      "the\n",
      "blind\n",
      "auditions\n",
      "for\n",
      "the\n",
      "season\n",
      "now\n",
      "being\n",
      "filmed\n",
      "with\n",
      "filming\n",
      "dates\n",
      "that\n",
      "were\n",
      "moved\n",
      "and\n",
      "was\n",
      "true\n",
      "that\n",
      "christina\n",
      "aguilera\n",
      "judging\n",
      "the\n",
      "voice\n",
      "why\n",
      "did\n",
      "ann7\n",
      "and\n",
      "infinity\n",
      "media\n",
      "choose\n",
      "her\n",
      "initially\n",
      "and\n",
      "not\n",
      "due\n",
      "diligence\n",
      "beforehand\n",
      "which\n",
      "would\n",
      "have\n",
      "revealed\n",
      "that\n",
      "christina\n",
      "aguilera\n",
      "not\n",
      "signable^\n",
      "for\n",
      "live\n",
      "performance\n",
      "south\n",
      "africa\n",
      "because\n",
      "she\n",
      "will\n",
      "judge\n",
      "all\n",
      "incredibly\n",
      "embarrassing\n",
      "for\n",
      "ann7\n",
      "the\n",
      "south\n",
      "african\n",
      "the\n",
      "year\n",
      "awards\n",
      "and\n",
      "for\n",
      "christina\n",
      "aguilera\n",
      "the\n",
      "reputation\n",
      "media\n",
      "outlet\n",
      "like\n",
      "ann7\n",
      "which\n",
      "the\n",
      "news\n",
      "business\n",
      "heavily\n",
      "based\n",
      "values\n",
      "like\n",
      "honesty\n",
      "transparency\n",
      "and\n",
      "above\n",
      "all\n",
      "credibility\n",
      "this\n",
      "latest\n",
      "sudden\n",
      "about-turn\n",
      "and\n",
      "having\n",
      "backtrack\n",
      "announcements\n",
      "and\n",
      "making\n",
      "mockery\n",
      "what\n",
      "the\n",
      "south\n",
      "african\n",
      "the\n",
      "year\n",
      "awards\n",
      "was\n",
      "announced\n",
      "initially\n",
      "once\n",
      "again\n",
      "dents\n",
      "and\n",
      "inflicts\n",
      "damage\n",
      "whatever\n",
      "credibility\n",
      "ann7\n",
      "has\n",
      "christina\n",
      "aguilera\n",
      "has\n",
      "had\n",
      "withdrawn\n",
      "due\n",
      "change\n",
      "scheduling\n",
      "the\n",
      "top-rated\n",
      "show\n",
      "the\n",
      "voice\n",
      "which\n",
      "she\n",
      "one\n",
      "the\n",
      "judges\n",
      "says\n",
      "ann7\n",
      "statement\n",
      "released\n",
      "friday\n",
      "morning\n",
      "ann7\n",
      "goes\n",
      "the\n",
      "schedule\n",
      "change\n",
      "requires\n",
      "her\n",
      "the\n",
      "united\n",
      "states\n",
      "the\n",
      "day\n",
      "the\n",
      "south\n",
      "african\n",
      "the\n",
      "year\n",
      "awards\n",
      "show\n",
      "nbc\n",
      "america\n",
      "has\n",
      "not\n",
      "made\n",
      "any\n",
      "schedule\n",
      "changes\n",
      "whatsoever\n",
      "its\n",
      "schedule\n",
      "for\n",
      "the\n",
      "upcoming\n",
      "season\n",
      "the\n",
      "voice\n",
      "which\n",
      "christina\n",
      "aguilera\n",
      "not\n",
      "judge\n",
      "ann7\n",
      "will\n",
      "now\n",
      "refunding\n",
      "all\n",
      "tickets\n",
      "those\n",
      "who\n",
      "already\n",
      "bought\n",
      "tickets\n",
      "for\n",
      "the\n",
      "south\n",
      "african\n",
      "the\n",
      "year\n",
      "awards\n",
      "set\n",
      "for\n",
      "october\n",
      "the\n",
      "ticketpro^\n",
      "dome\n",
      "randburg\n",
      "johannesburg\n",
      "the\n",
      "south\n",
      "african\n",
      "the\n",
      "year\n",
      "awards\n",
      "will\n",
      "continue\n",
      "but\n",
      "will\n",
      "now\n",
      "suddenly\n",
      "become\n",
      "private\n",
      "event\n",
      "ann7\n",
      "says\n",
      "anyone\n",
      "who\n",
      "has\n",
      "not\n",
      "heard\n",
      "from\n",
      "ticketpro^\n",
      "should\n",
      "call\n",
      "their\n",
      "call\n",
      "centre\n",
      "ensure\n",
      "prompt\n",
      "refunding\n",
      "takes\n",
      "place\n",
      "moegsien^\n",
      "williams\n",
      "editor-in-chief\n",
      "for\n",
      "ann7\n",
      "and\n",
      "saty2015^\n",
      "organiser\n",
      "says\n",
      "statement\n",
      "ann7\n",
      "deeply\n",
      "disappointed\n",
      "that\n",
      "will\n",
      "suddenly\n",
      "longer\n",
      "able\n",
      "bring\n",
      "christina\n",
      "aguilera\n",
      "south\n",
      "africa\n",
      "but\n",
      "that\n",
      "ann7\n",
      "understands\n",
      "that\n",
      "she\n",
      "has\n",
      "contractual\n",
      "obligations\n",
      "that\n",
      "she\n",
      "must\n",
      "meet\n",
      "the\n",
      "south\n",
      "african\n",
      "the\n",
      "year\n",
      "awards\n",
      "will\n",
      "still\n",
      "live\n",
      "broadcast\n",
      "the\n",
      "channel\n",
      "october\n"
     ]
    }
   ],
   "source": [
    "i = 334\n",
    "prt('H',Y_test[i])\n",
    "prt('D',X_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout, RepeatVector\n",
    "from keras.layers import Merge, SpatialDropout1D\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# seed weight initialization\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regularizer = l2(weight_decay) if weight_decay else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#keras.layers.embeddings.Embedding(input_dim, output_dim, embeddings_initializer='uniform', \n",
    "#            embeddings_regularizer=None, activity_regularizer=None, \n",
    "#            embeddings_constraint=None, mask_zero=False, input_length=None)\n",
    "model.add(Embedding(input_dim=vocab_size, \n",
    "                    output_dim=embedding_size,\n",
    "                    embeddings_initializer='uniform',\n",
    "                    embeddings_regularizer=regularizer,\n",
    "                    mask_zero=True,\n",
    "                    input_length=maxlen,\n",
    "                    name='embedding_1'))\n",
    "\n",
    "model.add(SpatialDropout1D(p_emb,\n",
    "                    name='dropout_emb_1'))\n",
    "\n",
    "for i in range(rnn_layers):\n",
    "    #New\n",
    "    #keras.layers.recurrent.LSTM(units, activation='tanh', recurrent_activation='hard_sigmoid', \n",
    "    #     use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', \n",
    "    #     bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, \n",
    "    #     bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, \n",
    "    #     bias_constraint=None, dropout=0.0, recurrent_dropout=0.0)\n",
    "    \n",
    "    lstm = LSTM(units=rnn_size, #units\n",
    "                return_sequences=True,\n",
    "                kernel_regularizer=regularizer, #kernel_regularizer\n",
    "                recurrent_regularizer=regularizer, #recurrent_regularizer\n",
    "                bias_regularizer=regularizer, #bias_regularizer\n",
    "                dropout=p_W, #dropout\n",
    "                recurrent_dropout=p_U, #recurrent_dropout\n",
    "                name='lstm_%d'%(i+1)\n",
    "                  )\n",
    "    model.add(lstm)\n",
    "    model.add(Dropout(p_dense,name='dropout_%d'%(i+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Lambda\n",
    "import keras.backend as K\n",
    "\n",
    "def simple_context(X, mask, n=activation_rnn_size, maxlend=maxlend, maxlenh=maxlenh):\n",
    "    print(type(mask))\n",
    "    desc, head = X[:,:maxlend,:], X[:,maxlend:,:]\n",
    "    head_activations, head_words = head[:,:,:n], head[:,:,n:]\n",
    "    desc_activations, desc_words = desc[:,:,:n], desc[:,:,n:]\n",
    "    \n",
    "    # RTFM http://deeplearning.net/software/theano/library/tensor/basic.html#theano.tensor.batched_tensordot\n",
    "    # activation for every head word and every desc word\n",
    "    activation_energies = K.batch_dot(head_activations, desc_activations, axes=(2,2))\n",
    "    # make sure we dont use description words that are masked out\n",
    "    if mask is not None:\n",
    "        activation_energies = activation_energies + -1e20*K.expand_dims(1.-K.cast(mask[:, :maxlend],'float32'),1)\n",
    "    \n",
    "    # for every head word compute weights for every desc word\n",
    "    activation_energies = K.reshape(activation_energies,(-1,maxlend))\n",
    "    activation_weights = K.softmax(activation_energies)\n",
    "    activation_weights = K.reshape(activation_weights,(-1,maxlenh,maxlend))\n",
    "\n",
    "    # for every head word compute weighted average of desc words\n",
    "    desc_avg_word = K.batch_dot(activation_weights, desc_words, axes=(2,1))\n",
    "    return K.concatenate((desc_avg_word, head_words))\n",
    "\n",
    "\n",
    "class SimpleContext(Lambda):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(SimpleContext, self).__init__(simple_context,**kwargs)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return input_mask[:, maxlend:]\n",
    "    \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        nb_samples = input_shape[0]\n",
    "        n = 2*(rnn_size - activation_rnn_size)\n",
    "        return (nb_samples, maxlenh, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "<class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "if activation_rnn_size:\n",
    "    model.add(SimpleContext(name='simplecontext_1'))\n",
    "\n",
    "#keras.layers.core.Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', \n",
    "#     bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, \n",
    "#     kernel_constraint=None, bias_constraint=None)\n",
    "model.add(TimeDistributed(Dense(units=vocab_size,\n",
    "                                kernel_regularizer=regularizer, \n",
    "                                bias_regularizer=regularizer,\n",
    "                                name = 'timedistributed_1')))\n",
    "model.add(Activation('softmax', name='activation_1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, RMSprop # usually I prefer Adam but article used rmsprop\n",
    "# opt = Adam(lr=LR)  # keep calm and reduce learning rate\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "// new Audio(\"http://www.soundjay.com/button/beep-09.wav\").play ()"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "// new Audio(\"http://www.soundjay.com/button/beep-09.wav\").play ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.set_value(model.optimizer.lr,np.float32(LR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_shape(x):\n",
    "    return 'x'.join(map(str,x.shape))\n",
    "    \n",
    "def inspect_model(model):\n",
    "    for i,l in enumerate(model.layers):\n",
    "        print (i, 'cls=%s name=%s'%(type(l).__name__, l.name))\n",
    "        weights = l.get_weights()\n",
    "        for weight in weights:\n",
    "            print (str_shape(weight))\n",
    "        #print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cls=Embedding name=embedding_1\n",
      "200000x100\n",
      "1 cls=SpatialDropout1D name=dropout_emb_1\n",
      "2 cls=LSTM name=lstm_1\n",
      "100x2048\n",
      "512x2048\n",
      "2048\n",
      "3 cls=Dropout name=dropout_1\n",
      "4 cls=LSTM name=lstm_2\n",
      "512x2048\n",
      "512x2048\n",
      "2048\n",
      "5 cls=Dropout name=dropout_2\n",
      "6 cls=LSTM name=lstm_3\n",
      "512x2048\n",
      "512x2048\n",
      "2048\n",
      "7 cls=Dropout name=dropout_3\n",
      "8 cls=SimpleContext name=simplecontext_1\n",
      "9 cls=TimeDistributed name=time_distributed_2\n",
      "944x200000\n",
      "200000\n",
      "10 cls=Activation name=activation_1\n"
     ]
    }
   ],
   "source": [
    "inspect_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (Unable to open file: name = 'data/train.hdf5', errno = 2, error message = 'no such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-d54ff88f5506>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mFN1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/%s.hdf5'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mFN1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/envs/tf/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m    713\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`load_weights` requires h5py.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'layer_names'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'model_weights'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/private/var/folders/my/m6ynh3bn6tq06h7xr3js0z7r0000gn/T/pip-gkjbrkhs-build/h5py/_objects.c:2840)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/private/var/folders/my/m6ynh3bn6tq06h7xr3js0z7r0000gn/T/pip-gkjbrkhs-build/h5py/_objects.c:2798)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open (/private/var/folders/my/m6ynh3bn6tq06h7xr3js0z7r0000gn/T/pip-gkjbrkhs-build/h5py/h5f.c:2117)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (Unable to open file: name = 'data/train.hdf5', errno = 2, error message = 'no such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "if FN1:\n",
    "    model.load_weights('data/%s.hdf5'%FN1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lpadd(x, maxlend=maxlend, eos=eos):\n",
    "    \"\"\"left (pre) pad a description to maxlend and then add eos.\n",
    "    The eos is the input to predicting the first word in the headline\n",
    "    \"\"\"\n",
    "    assert maxlend >= 0\n",
    "    if maxlend == 0:\n",
    "        return [eos]\n",
    "    n = len(x)\n",
    "    if n > maxlend:\n",
    "        x = x[-maxlend:]\n",
    "        n = maxlend\n",
    "    return [empty]*(maxlend-n) + x + [eos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples = [lpadd([3]*26)]\n",
    "# pad from right (post) so the first maxlend will be description followed by headline\n",
    "data = sequence.pad_sequences(samples, maxlen=maxlen, value=empty, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(data[:,maxlend] == eos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 50), <map at 0x2eb504cf8>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape,map(len, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 25, 200000)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = model.predict(data, verbose=0, batch_size=1)\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# variation to https://github.com/ryankiros/skip-thoughts/blob/master/decoding/search.py\n",
    "def beamsearch(predict, start=[empty]*maxlend + [eos],\n",
    "               k=1, maxsample=maxlen, use_unk=True, empty=empty, eos=eos, temperature=1.0):\n",
    "    \"\"\"return k samples (beams) and their NLL scores, each sample is a sequence of labels,\n",
    "    all samples starts with an `empty` label and end with `eos` or truncated to length of `maxsample`.\n",
    "    You need to supply `predict` which returns the label probability of each sample.\n",
    "    `use_unk` allow usage of `oov` (out-of-vocabulary) label in samples\n",
    "    \"\"\"\n",
    "    def sample(energy, n, temperature=temperature):\n",
    "        \"\"\"sample at most n elements according to their energy\"\"\"\n",
    "        n = min(n,len(energy))\n",
    "        prb = np.exp(-np.array(energy) / temperature )\n",
    "        res = []\n",
    "        for i in xrange(n):\n",
    "            z = np.sum(prb)\n",
    "            r = np.argmax(np.random.multinomial(1, prb/z, 1))\n",
    "            res.append(r)\n",
    "            prb[r] = 0. # make sure we select each element only once\n",
    "        return res\n",
    "\n",
    "    dead_k = 0 # samples that reached eos\n",
    "    dead_samples = []\n",
    "    dead_scores = []\n",
    "    live_k = 1 # samples that did not yet reached eos\n",
    "    live_samples = [list(start)]\n",
    "    live_scores = [0]\n",
    "\n",
    "    while live_k:\n",
    "        # for every possible live sample calc prob for every possible label \n",
    "        probs = predict(live_samples, empty=empty)\n",
    "\n",
    "        # total score for every sample is sum of -log of word prb\n",
    "        cand_scores = np.array(live_scores)[:,None] - np.log(probs)\n",
    "        cand_scores[:,empty] = 1e20\n",
    "        if not use_unk:\n",
    "            for i in range(nb_unknown_words):\n",
    "                cand_scores[:,vocab_size - 1 - i] = 1e20\n",
    "        live_scores = list(cand_scores.flatten())\n",
    "        \n",
    "\n",
    "        # find the best (lowest) scores we have from all possible dead samples and\n",
    "        # all live samples and all possible new words added\n",
    "        scores = dead_scores + live_scores\n",
    "        ranks = sample(scores, k)\n",
    "        n = len(dead_scores)\n",
    "        ranks_dead = [r for r in ranks if r < n]\n",
    "        ranks_live = [r - n for r in ranks if r >= n]\n",
    "        \n",
    "        dead_scores = [dead_scores[r] for r in ranks_dead]\n",
    "        dead_samples = [dead_samples[r] for r in ranks_dead]\n",
    "        \n",
    "        live_scores = [live_scores[r] for r in ranks_live]\n",
    "\n",
    "        # append the new words to their appropriate live sample\n",
    "        voc_size = probs.shape[1]\n",
    "        live_samples = [live_samples[r//voc_size]+[r%voc_size] for r in ranks_live]\n",
    "\n",
    "        # live samples that should be dead are...\n",
    "        # even if len(live_samples) == maxsample we dont want it dead because we want one\n",
    "        # last prediction out of it to reach a headline of maxlenh\n",
    "        zombie = [s[-1] == eos or len(s) > maxsample for s in live_samples]\n",
    "        \n",
    "        # add zombies to the dead\n",
    "        dead_samples += [s for s,z in zip(live_samples,zombie) if z]\n",
    "        dead_scores += [s for s,z in zip(live_scores,zombie) if z]\n",
    "        dead_k = len(dead_samples)\n",
    "        # remove zombies from the living \n",
    "        live_samples = [s for s,z in zip(live_samples,zombie) if not z]\n",
    "        live_scores = [s for s,z in zip(live_scores,zombie) if not z]\n",
    "        live_k = len(live_samples)\n",
    "\n",
    "    return dead_samples + live_samples, dead_scores + live_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keras_rnn_predict(samples, empty=empty, model=model, maxlen=maxlen):\n",
    "    \"\"\"for every sample, calculate probability for every possible label\n",
    "    you need to supply your RNN model and maxlen - the length of sequences it can handle\n",
    "    \"\"\"\n",
    "    sample_lengths = map(len, samples)\n",
    "    assert all(l > maxlend for l in sample_lengths)\n",
    "    assert all(l[maxlend] == eos for l in samples)\n",
    "    # pad from right (post) so the first maxlend will be description followed by headline\n",
    "    data = sequence.pad_sequences(samples, maxlen=maxlen, value=empty, padding='post', truncating='post')\n",
    "    probs = model.predict(data, verbose=0, batch_size=batch_size)\n",
    "    return np.array([prob[sample_length-maxlend-1] for prob, sample_length in zip(probs, sample_lengths)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vocab_fold(xs):\n",
    "    \"\"\"convert list of word indexes that may contain words outside vocab_size to words inside.\n",
    "    If a word is outside, try first to use glove_idx2idx to find a similar word inside.\n",
    "    If none exist then replace all accurancies of the same unknown word with <0>, <1>, ...\n",
    "    \"\"\"\n",
    "    xs = [x if x < oov0 else glove_idx2idx.get(x,x) for x in xs]\n",
    "    # the more popular word is <0> and so on\n",
    "    outside = sorted([x for x in xs if x >= oov0])\n",
    "    # if there are more than nb_unknown_words oov words then put them all in nb_unknown_words-1\n",
    "    outside = dict((x,vocab_size-1-min(i, nb_unknown_words-1)) for i, x in enumerate(outside))\n",
    "    xs = [outside.get(x,x) for x in xs]\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vocab_unfold(desc,xs):\n",
    "    # assume desc is the unfolded version of the start of xs\n",
    "    unfold = {}\n",
    "    for i, unfold_idx in enumerate(desc):\n",
    "        fold_idx = xs[i]\n",
    "        if fold_idx >= oov0:\n",
    "            unfold[fold_idx] = unfold_idx\n",
    "    return [unfold.get(x,x) for x in xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import Levenshtein\n",
    "\n",
    "def gensamples(skips=2, k=10, batch_size=batch_size, short=True, temperature=1., use_unk=True):\n",
    "    i = random.randint(0,len(X_test)-1)\n",
    "    print ('HEAD:',' '.join(idx2word[w] for w in Y_test[i][:maxlenh]))\n",
    "    print ('DESC:',' '.join(idx2word[w] for w in X_test[i][:maxlend]))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    print ('HEADS:')\n",
    "    x = X_test[i]\n",
    "    samples = []\n",
    "    if maxlend == 0:\n",
    "        skips = [0]\n",
    "    else:\n",
    "        skips = range(min(maxlend,len(x)), max(maxlend,len(x)), abs(maxlend - len(x)) // skips + 1)\n",
    "    for s in skips:\n",
    "        start = lpadd(x[:s])\n",
    "        fold_start = vocab_fold(start)\n",
    "        sample, score = beamsearch(predict=keras_rnn_predict, start=fold_start, k=k, temperature=temperature, use_unk=use_unk)\n",
    "        assert all(s[maxlend] == eos for s in sample)\n",
    "        samples += [(s,start,scr) for s,scr in zip(sample,score)]\n",
    "\n",
    "    samples.sort(key=lambda x: x[-1])\n",
    "    codes = []\n",
    "    for sample, start, score in samples:\n",
    "        code = ''\n",
    "        words = []\n",
    "        sample = vocab_unfold(start, sample)[len(start):]\n",
    "        for w in sample:\n",
    "            if w == eos:\n",
    "                break\n",
    "            words.append(idx2word[w])\n",
    "            code += chr(w//(256*256)) + chr((w//256)%256) + chr(w%256)\n",
    "        if short:\n",
    "            distance = min([100] + [-Levenshtein.jaro(code,c) for c in codes])\n",
    "            if distance > -0.6:\n",
    "                print (score, ' '.join(words))\n",
    "        #         print ('%s (%.2f) %f'%(' '.join(words), score, distance))\n",
    "        else:\n",
    "                print (score, ' '.join(words))\n",
    "        codes.append(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAD: lester piggott frankie frankel fairytales\n",
      "DESC: everything changes even everyday life progress lester piggott jockey how old now people whisper passing the tell-tale lines his well-weathered^ face give the game away\n",
      "HEADS:\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 1 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-f97810ccf9bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgensamples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskips\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-99-da015d07e185>\u001b[0m in \u001b[0;36mgensamples\u001b[0;34m(skips, k, batch_size, short, temperature, use_unk)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlpadd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mfold_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeamsearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras_rnn_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfold_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_unk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_unk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmaxlend\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0meos\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-93-547e53829781>\u001b[0m in \u001b[0;36mbeamsearch\u001b[0;34m(predict, start, k, maxsample, use_unk, empty, eos, temperature)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# total score for every sample is sum of -log of word prb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mcand_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlive_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mcand_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muse_unk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_unknown_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 1 with size 0"
     ]
    }
   ],
   "source": [
    "gensamples(skips=2, batch_size=batch_size, k=10, temperature=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_headline(x, nflips=None, model=None, debug=False):\n",
    "    \"\"\"given a vectorized input (after `pad_sequences`) flip some of the words in the second half (headline)\n",
    "    with words predicted by the model\n",
    "    \"\"\"\n",
    "    if nflips is None or model is None or nflips <= 0:\n",
    "        return x\n",
    "    \n",
    "    batch_size = len(x)\n",
    "    assert np.all(x[:,maxlend] == eos)\n",
    "    probs = model.predict(x, verbose=0, batch_size=batch_size)\n",
    "    x_out = x.copy()\n",
    "    for b in range(batch_size):\n",
    "        # pick locations we want to flip\n",
    "        # 0...maxlend-1 are descriptions and should be fixed\n",
    "        # maxlend is eos and should be fixed\n",
    "        flips = sorted(random.sample(range(maxlend+1,maxlen), nflips))\n",
    "        if debug and b < debug:\n",
    "            print(b)\n",
    "        for input_idx in flips:\n",
    "            if x[b,input_idx] == empty or x[b,input_idx] == eos:\n",
    "                continue\n",
    "            # convert from input location to label location\n",
    "            # the output at maxlend (when input is eos) is feed as input at maxlend+1\n",
    "            label_idx = input_idx - (maxlend+1)\n",
    "            prob = probs[b, label_idx]\n",
    "            w = prob.argmax()\n",
    "            if w == empty:  # replace accidental empty with oov\n",
    "                w = oov0\n",
    "            if debug and b < debug:\n",
    "                print ('%s => %s'%(idx2word[x_out[b,input_idx]],idx2word[w]))\n",
    "            x_out[b,input_idx] = w\n",
    "        #if debug and b < debug:\n",
    "        #    print\n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_seq_labels(xds, xhs, nflips=None, model=None, debug=False):\n",
    "    \"\"\"description and hedlines are converted to padded input vectors. headlines are one-hot to label\"\"\"\n",
    "    batch_size = len(xhs)\n",
    "    assert len(xds) == batch_size\n",
    "    x = [vocab_fold(lpadd(xd)+xh) for xd,xh in zip(xds,xhs)]  # the input does not have 2nd eos\n",
    "    x = sequence.pad_sequences(x, maxlen=maxlen, value=empty, padding='post', truncating='post')\n",
    "    x = flip_headline(x, nflips=nflips, model=model, debug=debug)\n",
    "    \n",
    "    y = np.zeros((batch_size, maxlenh, vocab_size))\n",
    "    for i, xh in enumerate(xhs):\n",
    "        xh = vocab_fold(xh) + [eos] + [empty]*maxlenh  # output does have a eos at end\n",
    "        xh = xh[:maxlenh]\n",
    "        y[i,:,:] = np_utils.to_categorical(xh, vocab_size)\n",
    "        \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen(Xd, Xh, batch_size=batch_size, nb_batches=None, nflips=None, model=None, debug=False, seed=seed):\n",
    "    \"\"\"yield batches. for training use nb_batches=None\n",
    "    for validation generate deterministic results repeating every nb_batches\n",
    "    \n",
    "    while training it is good idea to flip once in a while the values of the headlines from the\n",
    "    value taken from Xh to value generated by the model.\n",
    "    \"\"\"\n",
    "    c = nb_batches if nb_batches else 0\n",
    "    while True:\n",
    "        xds = []\n",
    "        xhs = []\n",
    "        if nb_batches and c >= nb_batches:\n",
    "            c = 0\n",
    "        new_seed = random.randint(0, sys.maxsize)\n",
    "        random.seed(c+123456789+seed)\n",
    "        for b in range(batch_size):\n",
    "            t = random.randint(0,len(Xd)-1)\n",
    "\n",
    "            xd = Xd[t]\n",
    "            s = random.randint(min(maxlend,len(xd)), max(maxlend,len(xd)))\n",
    "            xds.append(xd[:s])\n",
    "            \n",
    "            xh = Xh[t]\n",
    "            s = random.randint(min(maxlenh,len(xh)), max(maxlenh,len(xh)))\n",
    "            xhs.append(xh[:s])\n",
    "\n",
    "        # undo the seeding before we yield inorder not to affect the caller\n",
    "        c+= 1\n",
    "        random.seed(new_seed)\n",
    "\n",
    "        yield conv_seq_labels(xds, xhs, nflips=nflips, model=model, debug=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((64, 50), (64, 25, 200000), 2)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = next(gen(X_train, Y_train, batch_size=batch_size))\n",
    "r[0].shape, r[1].shape, len(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_gen(gen, n=5):\n",
    "    Xtr,Ytr = next(gen)\n",
    "    for i in range(n):\n",
    "        assert Xtr[i,maxlend] == eos\n",
    "        x = Xtr[i,:maxlend]\n",
    "        y = Xtr[i,maxlend:]\n",
    "        yy = Ytr[i,:]\n",
    "        yy = np.where(yy)[1]\n",
    "        prt('L',yy)\n",
    "        prt('H',y)\n",
    "        if maxlend:\n",
    "            prt('D',x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L:\n",
      "man\n",
      "dies\n",
      "after\n",
      "reportedly\n",
      "jumping\n",
      "central\n",
      "park\n",
      "reservoir\n",
      "~\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "H:\n",
      "~\n",
      "man\n",
      "dies\n",
      "after\n",
      "reportedly\n",
      "jumping\n",
      "central\n",
      "park\n",
      "reservoir\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "D:\n",
      "down\n",
      "<1>^\n",
      "reservoir\n",
      "nypd\n",
      "copter\n",
      "above\n",
      "least\n",
      "one\n",
      "diver\n",
      "into\n",
      "water\n",
      "multiple\n",
      "<3>^\n",
      "nick\n",
      "<0>^\n",
      "<2>^\n",
      "september\n",
      "police\n",
      "say\n",
      "the\n",
      "man\n",
      "was\n",
      "seen\n",
      "going\n",
      "into\n",
      "L:\n",
      "associations\n",
      "genetic\n",
      "variations\n",
      "<1>^\n",
      "<2>^\n",
      "and\n",
      "<3>^\n",
      "with\n",
      "noise-induced\n",
      "hearing\n",
      "loss\n",
      "chinese\n",
      "population\n",
      "<0>^\n",
      "control\n",
      "study\n",
      "~\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "H:\n",
      "~\n",
      "associations\n",
      "genetic\n",
      "variations\n",
      "<2>^\n",
      "<3>^\n",
      "and\n",
      "<4>^\n",
      "with\n",
      "noise-induced\n",
      "hearing\n",
      "loss\n",
      "chinese\n",
      "population\n",
      "<0>^\n",
      "control\n",
      "study\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "D:\n",
      "exposure\n",
      "the\n",
      "workers\n",
      "were\n",
      "excluded\n",
      "from\n",
      "the\n",
      "study\n",
      "their\n",
      "differences\n",
      "<1>^\n",
      "between\n",
      "left\n",
      "and\n",
      "right\n",
      "ears\n",
      "were\n",
      "great\n",
      "then\n",
      "the\n",
      "normal\n",
      "group\n",
      "included\n",
      "the\n",
      "workers\n",
      "L:\n",
      "investor\n",
      "adds\n",
      "wynwood\n",
      "assemblage\n",
      "for\n",
      "~\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "H:\n",
      "~\n",
      "investor\n",
      "adds\n",
      "wynwood\n",
      "assemblage\n",
      "for\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "D:\n",
      "corner\n",
      "northwest\n",
      "fifth\n",
      "avenue\n",
      "and\n",
      "northwest\n",
      "street\n",
      "miami\n",
      "wynwood\n",
      "neighborhood\n",
      "sold\n",
      "for\n",
      "million\n",
      "according\n",
      "miami-dade\n",
      "county\n",
      "records\n",
      "the\n",
      "betty\n",
      "simon\n",
      "revocable\n",
      "trust\n",
      "sold\n",
      "the\n",
      "building\n",
      "L:\n",
      "wireless\n",
      "sensor\n",
      "networks\n",
      "wsn\n",
      "forecasts\n",
      "technologies\n",
      "players\n",
      "~\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "H:\n",
      "~\n",
      "wireless\n",
      "sensor\n",
      "networks\n",
      "wsn\n",
      "forecasts\n",
      "technologies\n",
      "players\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "D:\n",
      "development\n",
      "and\n",
      "use\n",
      "wsn\n",
      "partly\n",
      "because\n",
      "the\n",
      "heavier\n",
      "funding\n",
      "available\n",
      "there\n",
      "industry\n",
      "sits\n",
      "astride\n",
      "the\n",
      "computer\n",
      "industry\n",
      "thanks\n",
      "companies\n",
      "such\n",
      "microsoft\n",
      "and\n",
      "ibm\n",
      "and\n",
      "wsn\n",
      "L:\n",
      "n't\n",
      "~\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "H:\n",
      "~\n",
      "n't\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "D:\n",
      "right\n",
      "reasons\n",
      "frequently\n",
      "too\n",
      "aloof\n",
      "when\n",
      "comes\n",
      "the\n",
      "machiavellian\n",
      "tendencies\n",
      "their\n",
      "coworkers\n",
      "least\n",
      "until\n",
      "too\n",
      "late\n",
      "otherwise\n",
      "powerless\n",
      "anything\n",
      "about\n",
      "hard\n",
      "remain\n",
      "committed\n",
      "once\n",
      "your\n"
     ]
    }
   ],
   "source": [
    "test_gen(gen(X_train, Y_train, batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Flipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L:\n",
      "man\n",
      "dies\n",
      "after\n",
      "reportedly\n",
      "jumping\n",
      "central\n",
      "park\n",
      "reservoir\n",
      "~\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "H:\n",
      "~\n",
      "iii-26\n",
      "dies\n",
      "after\n",
      "reportedly\n",
      "jumping\n",
      "central\n",
      "www.liolios.com\n",
      "reservoir\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "D:\n",
      "down\n",
      "<1>^\n",
      "reservoir\n",
      "nypd\n",
      "copter\n",
      "above\n",
      "least\n",
      "one\n",
      "diver\n",
      "into\n",
      "water\n",
      "multiple\n",
      "<3>^\n",
      "nick\n",
      "<0>^\n",
      "<2>^\n",
      "september\n",
      "police\n",
      "say\n",
      "the\n",
      "man\n",
      "was\n",
      "seen\n",
      "going\n",
      "into\n",
      "L:\n",
      "associations\n",
      "genetic\n",
      "variations\n",
      "<1>^\n",
      "<2>^\n",
      "and\n",
      "<3>^\n",
      "with\n",
      "noise-induced\n",
      "hearing\n",
      "loss\n",
      "chinese\n",
      "population\n",
      "<0>^\n",
      "control\n",
      "study\n",
      "~\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "H:\n",
      "~\n",
      "poppy\n",
      "genetic\n",
      "variations\n",
      "<2>^\n",
      "<3>^\n",
      "and\n",
      "solarvest\n",
      "imelda\n",
      "noise-induced\n",
      "hearing\n",
      "loss\n",
      "chinese\n",
      "population\n",
      "butti\n",
      "control\n",
      "study\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "D:\n",
      "exposure\n",
      "the\n",
      "workers\n",
      "were\n",
      "excluded\n",
      "from\n",
      "the\n",
      "study\n",
      "their\n",
      "differences\n",
      "<1>^\n",
      "between\n",
      "left\n",
      "and\n",
      "right\n",
      "ears\n",
      "were\n",
      "great\n",
      "then\n",
      "the\n",
      "normal\n",
      "group\n",
      "included\n",
      "the\n",
      "workers\n",
      "L:\n",
      "investor\n",
      "adds\n",
      "wynwood\n",
      "assemblage\n",
      "for\n",
      "~\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "H:\n",
      "~\n",
      "euro2016\n",
      "adds\n",
      "wynwood\n",
      "assemblage\n",
      "for\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "D:\n",
      "corner\n",
      "northwest\n",
      "fifth\n",
      "avenue\n",
      "and\n",
      "northwest\n",
      "street\n",
      "miami\n",
      "wynwood\n",
      "neighborhood\n",
      "sold\n",
      "for\n",
      "million\n",
      "according\n",
      "miami-dade\n",
      "county\n",
      "records\n",
      "the\n",
      "betty\n",
      "simon\n",
      "revocable\n",
      "trust\n",
      "sold\n",
      "the\n",
      "building\n",
      "L:\n",
      "wireless\n",
      "sensor\n",
      "networks\n",
      "wsn\n",
      "forecasts\n",
      "technologies\n",
      "players\n",
      "~\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "H:\n",
      "~\n",
      "wireless\n",
      "sensor\n",
      "networks\n",
      "wsn\n",
      "forecasts\n",
      "technologies\n",
      "players\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "D:\n",
      "development\n",
      "and\n",
      "use\n",
      "wsn\n",
      "partly\n",
      "because\n",
      "the\n",
      "heavier\n",
      "funding\n",
      "available\n",
      "there\n",
      "industry\n",
      "sits\n",
      "astride\n",
      "the\n",
      "computer\n",
      "industry\n",
      "thanks\n",
      "companies\n",
      "such\n",
      "microsoft\n",
      "and\n",
      "ibm\n",
      "and\n",
      "wsn\n",
      "L:\n",
      "n't\n",
      "~\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "H:\n",
      "~\n",
      "nehemiah\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "D:\n",
      "right\n",
      "reasons\n",
      "frequently\n",
      "too\n",
      "aloof\n",
      "when\n",
      "comes\n",
      "the\n",
      "machiavellian\n",
      "tendencies\n",
      "their\n",
      "coworkers\n",
      "least\n",
      "until\n",
      "too\n",
      "late\n",
      "otherwise\n",
      "powerless\n",
      "anything\n",
      "about\n",
      "hard\n",
      "remain\n",
      "committed\n",
      "once\n",
      "your\n"
     ]
    }
   ],
   "source": [
    "test_gen(gen(X_train, Y_train, nflips=6, model=model, debug=False, batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valgen = gen(X_test, Y_test,nb_batches=3, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that valgen repeats itself after nb_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L:\n",
      "state\n",
      "troopers\n",
      "injured\n",
      "during\n",
      "texas\n",
      "panhandle\n",
      "traffic\n",
      "stop\n",
      "~\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "H:\n",
      "~\n",
      "state\n",
      "troopers\n",
      "injured\n",
      "during\n",
      "texas\n",
      "panhandle\n",
      "traffic\n",
      "stop\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "D:\n",
      "the\n",
      "driver\n",
      "got\n",
      "back\n",
      "into\n",
      "the\n",
      "car\n",
      "and\n",
      "tried\n",
      "drive\n",
      "away\n",
      "trooper\n",
      "tried\n",
      "place\n",
      "the\n",
      "car\n",
      "park\n",
      "and\n",
      "remove\n",
      "the\n",
      "keys\n",
      "but\n",
      "fell\n",
      "out\n",
      "the\n",
      "L:\n",
      "sgoco\n",
      "group\n",
      "ltd\n",
      "announces\n",
      "has\n",
      "retained\n",
      "nyse\n",
      "member\n",
      "firm\n",
      "the\n",
      "company\n",
      "strategic\n",
      "~\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "H:\n",
      "~\n",
      "sgoco\n",
      "group\n",
      "ltd\n",
      "announces\n",
      "has\n",
      "retained\n",
      "nyse\n",
      "member\n",
      "firm\n",
      "the\n",
      "company\n",
      "strategic\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "D:\n",
      "brand\n",
      "development\n",
      "the\n",
      "chinese\n",
      "display\n",
      "market\n",
      "today\n",
      "announced\n",
      "hiring\n",
      "leading\n",
      "new\n",
      "york\n",
      "based\n",
      "investment\n",
      "banking\n",
      "firm\n",
      "the\n",
      "company\n",
      "new\n",
      "strategic\n",
      "advisors\n",
      "the\n",
      "company\n",
      "along\n",
      "exploring\n",
      "L:\n",
      "photos\n",
      "usd\n",
      "vs.\n",
      "kansas\n",
      "state\n",
      "football\n",
      "~\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "H:\n",
      "~\n",
      "photos\n",
      "usd\n",
      "vs.\n",
      "kansas\n",
      "state\n",
      "football\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "D:\n",
      "tackled\n",
      "kansas\n",
      "state\n",
      "defensive\n",
      "tackles\n",
      "will\n",
      "geary\n",
      "travis\n",
      "rossouw\n",
      "and\n",
      "linebacker\n",
      "elijah\n",
      "lee\n",
      "back\n",
      "right\n",
      "during\n",
      "the\n",
      "first\n",
      "half\n",
      "ncaa\n",
      "college\n",
      "football\n",
      "game\n",
      "manhattan\n",
      "kan.\n",
      "L:\n",
      "state\n",
      "troopers\n",
      "injured\n",
      "during\n",
      "texas\n",
      "panhandle\n",
      "traffic\n",
      "stop\n",
      "~\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "H:\n",
      "~\n",
      "state\n",
      "troopers\n",
      "injured\n",
      "during\n",
      "texas\n",
      "panhandle\n",
      "traffic\n",
      "stop\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "_\n",
      "D:\n",
      "the\n",
      "driver\n",
      "got\n",
      "back\n",
      "into\n",
      "the\n",
      "car\n",
      "and\n",
      "tried\n",
      "drive\n",
      "away\n",
      "trooper\n",
      "tried\n",
      "place\n",
      "the\n",
      "car\n",
      "park\n",
      "and\n",
      "remove\n",
      "the\n",
      "keys\n",
      "but\n",
      "fell\n",
      "out\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    test_gen(valgen, n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "traingen = gen(X_train, Y_train, batch_size=batch_size, nflips=nflips, model=model)\n",
    "valgen = gen(X_test, Y_test, nb_batches=nb_val_samples//batch_size, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krotovd/anaconda/envs/tf/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: The semantics of the Keras 2 argument  `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Update your method calls accordingly.\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/krotovd/anaconda/envs/tf/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., validation_data=<generator..., steps_per_epoch=600000, epochs=1, validation_steps=100000)`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "     3/600000 [..............................] - ETA: 99954868s - loss: 12.2054 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-171e40f4ffb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Iteration'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     h = model.fit_generator(traingen, samples_per_epoch=nb_train_samples,\n\u001b[0;32m----> 4\u001b[0;31m                         \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_val_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_val_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m                            )\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1122\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m                                         \u001b[0mpickle_safe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1900\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1901\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1902\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1904\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1640\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1642\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1643\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2269\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2270\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/anaconda/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for iteration in range(500):\n",
    "    print ('Iteration', iteration)\n",
    "    #fit_generator(self, generator, steps_per_epoch, epochs=1, verbose=1, callbacks=None, \n",
    "    #     validation_data=None, validation_steps=None, class_weight=None, max_q_size=10, \n",
    "    #     workers=1, pickle_safe=False, initial_epoch=0)\n",
    "    h = model.fit_generator(generator=traingen, \n",
    "                            steps_per_epoch=nb_train_samples,\n",
    "                            epochs=1, \n",
    "                            validation_data=valgen, \n",
    "                            validation_steps=nb_val_samples\n",
    "                           )\n",
    "    for k,v in h.history.iteritems():\n",
    "        history[k] = history.get(k,[]) + v\n",
    "    with open('data/%s.history.pkl'%FN,'wb') as fp:\n",
    "        pickle.dump(history,fp,-1)\n",
    "    model.save_weights('data/%s.hdf5'%FN, overwrite=True)\n",
    "    gensamples(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
